"""
Module for training and tuning MLPR with dadi-simulated data
"""
from multiprocessing import Pool
import math
import numpy as np
from sklearn.neural_network import MLPRegressor
from sklearn.experimental import enable_halving_search_cv  # noqa
from sklearn.model_selection import HalvingRandomSearchCV, cross_val_score
from mapie.regression import MapieRegressor


def prep_data(data: dict, mapie=True):
    '''
    Helper method for outputing X and y from input data dict
    Input: data dict generated by generate_fs() method
    Output: X_input as a list of flattened fs datasets
            y_label_unpack as a list of list, where each inner list
            is the label of one single demographic param if mapie,
            If mapie=False, y_label_unpack will be a list of one list,
            with this one inner list containing tuples of all dem params.
    '''

    # require dict to be ordered (Python 3.7+)
    X_input = [np.array(fs).flatten() for fs in data.values()]
    y_label = list(data.keys())

    # # longer implementation so as to not rely on data dict being ordered
    # X_input, y_label = [], []
    # for params, fs in data.items():
    #     X_input.append(np.array(fs).flatten())
    #     y_label.append(params)

    # parse labels into single list for each param (required for mapie)
    y_label_unpack = list(zip(*y_label)) if mapie else [y_label]

    return X_input, y_label_unpack


def _tune_worker_func(args):
    '''
    Helper method for tuning() parallelization with Pool
    '''
    X_input, param, mlpr, param_dist, eta, max_iter, n_iters, cv = args
    search = HalvingRandomSearchCV(mlpr, param_dist,
                                   resource='max_iter', factor=eta,
                                   max_resources=max_iter,
                                   min_resources=n_iters, cv=cv,
                                   refit=False, n_jobs=1)
    # note: resource is defined by max_iter rather than n_samples
    search.fit(X_input, param)
    return [search, param]


def tune(X_input, y_label, param_dist, max_iter=243, eta=3, cv=5):
    '''
    Method for searching over many MLPR hyperparameters
    with successive halving randomized search and hyperband
    Input:
        X_input: list of fs data sets from _prep_data()
        y_label: list of list of unpacked param labels from _prep_data()
        param_dist: param distribution dictionary specifying values of
            hyperparams to search over
        max_iter: maximum iterations (lbfgs) or epochs (adam) allowed
            per candidate, ideally a power of eta, e.g. 81 = 3^4
        eta: the halving parameter (factor)
        cv: k-fold of stratified cross validation
    Output: list of search results for each mlpr model
        Note: len(result_list) = len(y_label)
    '''

    s_max = int(math.log(max_iter)/math.log(eta))
    # number of unique executions of Successive Halving (minus one)

    result_list = []

    # parallelize with Pool
    search_dict = {}
    args_list = []
    for param in y_label:
        search_dict[param] = []
        # begin Hyperband outer loop
        n_iter_list = [int(max_iter*eta**(-s))
                       for s in list(reversed(range(s_max+1)))]
        for ii, _ in enumerate(n_iter_list):
            args_list.append((X_input, param, MLPRegressor(),
                              param_dist, eta, max_iter, n_iter_list[ii], cv,))

    with Pool(processes=len(n_iter_list)*len(y_label)) as pool:
        res = pool.map(_tune_worker_func, args_list)
    for search, param in res:
        search_dict[param].append(search)

    for param in y_label:
        result_list.append(search_dict[param])

    return result_list


def _train_worker_func(args):
    '''
    Helper method for tuning() parallelization with Pool
    '''
    mlpr_spec, mapie, X_input, param = args
    # initiate one mlpr with mlpr_spec for each mlpr model
    mlpr = MLPRegressor()
    mlpr.set_params(**mlpr_spec)

    # use sklearn mlpr with multi-output option or mapie mlpr
    param_predictor = MapieRegressor(mlpr) if mapie else mlpr

    # train mlpr with input data
    param_predictor.fit(X_input, param)
    return [param_predictor, param]


def train(X_input, y_label, mlpr_specs, mapie=True) -> list:
    '''
    Train one or multiple MLPR for one demographic model data set
    with either mapie or sklearn MLPR
    Input:
        X_input: list of fs data sets from _prep_data()
        y_label: list of list of unpacked param labels from _prep_data()
        mlpr_specs: list of dictionary of MLPR architecture specifications
            Note: len(y_label) = len(mlpr_spec) = len(mlpr_list)
        mapie: if False will use sklearn mlpr with multioutput option
        if True (default) will use mapie mlpr with single-output option
    Output:
        List of trained MLPR model(s)
        Single model if using sklearn, multiple models if using mapie
        Order of models in list is the same as order of dem parameters
    '''

    mlpr_list = []

    param_predictor_dict = {}
    args_list = []
    for param, mlpr_spec in zip(y_label, mlpr_specs):
        args_list.append((mlpr_spec, mapie, X_input, param))
        # param is a tuple of len(data) for one dem param if mapie
        # or a list of len(data) for all params tuples if sklearn
    with Pool(processes=len(y_label)) as pool:
        res = pool.map(_train_worker_func, args_list)
    for param_predictor, param in res:
        param_predictor_dict[param] = param_predictor
    for param in y_label:
        # save trained mlpr model
        mlpr_list.append(param_predictor_dict[param])

    return mlpr_list


def report(results, file_handle, n_top=3):
    '''Utility function to report best scores'''

    for i in range(1, n_top + 1):
        candidates = np.flatnonzero(results["rank_test_score"] == i)
        for candidate in candidates:
            print("Model with rank: {0}".format(i), file=file_handle)
            print(
                "Mean validation score: {0:.3f} (std: {1:.3f})".format(
                    results["mean_test_score"][candidate],
                    results["std_test_score"][candidate],),
                file=file_handle)
            print("Parameters: {0}\n".format(results["params"][candidate]),
                  file=file_handle)


def get_best_specs(result_list):
    '''
    Helper method to extract best mlpr specs dict from the full result
    Input:
        result_list from tune() method
    Output:
        list of dict, where each dict is a mlpr_spec of the best mlpr
    '''

    mlpr_specs = []
    score_list = []
    # iterate through all successive halving searches to get best mlpr params
    for search_list in result_list:  # repeat for each mlpr model
        best_scores = [search.best_score_ for search in search_list]
        best_params = [search.best_params_ for search in search_list]
        best_score = max(best_scores)
        best_index = best_scores.index(best_score)
        mlpr_specs.append(best_params[best_index])
        score_list.append(best_score)

    return mlpr_specs, score_list


def get_cv_score(mlpr_list, X_input, y_label, file_handle, cv=5):
    '''Helper method for getting cross-validation score
    on training set after the mlpr is trained'''
    for i, (y_param, mlpr) in enumerate(zip(y_label, mlpr_list)):
        index = 'all' if len(mlpr_list) == 1 else i+1
        print(f'\nCV scores of best MLPR for param {index}:', file=file_handle)
        param_score = cross_val_score(mlpr, X_input, y_param, cv=cv, n_jobs=-1)
        for j, score in enumerate(param_score):
            print(f'[CV {j+1}/] score: {score}', file=file_handle)
