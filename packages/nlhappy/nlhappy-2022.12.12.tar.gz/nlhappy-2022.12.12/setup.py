# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['nlhappy',
 'nlhappy.algorithms',
 'nlhappy.callbacks',
 'nlhappy.configs',
 'nlhappy.data',
 'nlhappy.datamodules',
 'nlhappy.layers',
 'nlhappy.layers.attention',
 'nlhappy.layers.classifier',
 'nlhappy.layers.embedding',
 'nlhappy.metrics',
 'nlhappy.models',
 'nlhappy.models.entity_extraction',
 'nlhappy.models.event_extraction',
 'nlhappy.models.prompt_relation_extraction',
 'nlhappy.models.question_answering',
 'nlhappy.models.relation_extraction',
 'nlhappy.models.span_extraction',
 'nlhappy.models.text_classification',
 'nlhappy.models.text_multi_classification',
 'nlhappy.models.text_pair_classification',
 'nlhappy.models.text_pair_regression',
 'nlhappy.models.token_classification',
 'nlhappy.tricks',
 'nlhappy.utils']

package_data = \
{'': ['*'],
 'nlhappy.configs': ['callbacks/*',
                     'datamodule/*',
                     'log_dir/*',
                     'logger/*',
                     'model/*',
                     'trainer/*']}

install_requires = \
['datasets>=2.0.0',
 'hydra-colorlog>=1.1.0',
 'hydra-core==1.1',
 'pydantic==1.10.2',
 'pytorch-lightning>=1.6.5',
 'rich>=12.4.3,<13.0.0',
 'srsly>=2.4.5',
 'torch>=1.8.0',
 'transformers>=4.17.0']

extras_require = \
{':extra == "all"': ['googletrans==4.0.0rc1', 'wandb>=0.12.18']}

entry_points = \
{'console_scripts': ['nlhappy = nlhappy.run:train']}

setup_kwargs = {
    'name': 'nlhappy',
    'version': '2022.12.12',
    'description': 'è‡ªç„¶è¯­è¨€å¤„ç†(NLP)',
    'long_description': '\n<div align=\'center\'>\n\n# NLHappy\n<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>\n<a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>\n<a href="https://hydra.cc/"><img alt="Config: Hydra" src="https://img.shields.io/badge/Config-Hydra-89b8cd"></a>\n<a href="https://github.com/ashleve/lightning-hydra-template"><img alt="Template" src="https://img.shields.io/badge/-Lightning--Hydra--Template-017F2F?style=flat&logo=github&labelColor=gray"></a>\n<a href="https://spacy.io/"><img alt="Spacy" src="https://img.shields.io/badge/component-%20Spacy-blue"></a>\n<a href="https://wandb.ai/"><img alt="WanDB" src="https://img.shields.io/badge/Log-WanDB-brightgreen"></a>\n</div>\n<br><br>\n\n## ğŸ“Œ&nbsp;&nbsp; ç®€ä»‹\n\nnlhappyè‡´åŠ›äºå¿«é€Ÿå®ŒæˆNLPä»»åŠ¡,ä½ å”¯ä¸€éœ€è¦åšçš„å°±æ˜¯å°†æ•°æ®å¤„ç†ä¸ºä»»åŠ¡å¯¹åº”çš„æ•°æ®ç±».\n> å®ƒä¸»è¦çš„ä¾èµ–æœ‰\n- [transformers](https://huggingface.co/docs/transformers/index): ä¸‹è½½é¢„è®­ç»ƒæƒé‡\n- [pytorch-lightning](https://pytorch-lightning.readthedocs.io/en/latest/): æ¨¡å‹è®­ç»ƒ\n- [datasets](https://huggingface.co/docs/datasets/index): æ„å»ºæ•°æ®é›†\n- [pydantic](https://wandb.ai/): æ„å»ºæ•°æ®æ¨¡å‹\n\n\n## ğŸš€&nbsp;&nbsp; å®‰è£…\n<details>\n<summary><b>å®‰è£…nlhappy</b></summary>\n\n> æ¨èå…ˆå»[pytorchå®˜ç½‘](https://pytorch.org/get-started/locally/)å®‰è£…pytorchå’Œå¯¹åº”cuda\n```bash\n# pip å®‰è£…\npip install --upgrade pip\npip install --upgrade nlhappy\n```\n</details>\n\n<details>\n<summary><b>å…¶ä»–å¯é€‰</b></summary>\n\n> æ¨èå®‰è£…wandbç”¨äºå¯è§†åŒ–è®­ç»ƒæ—¥å¿—\n- æ³¨å†Œ: https://wandb.ai/\n- è·å–è®¤è¯: https://wandb.ai/authorize\n- ç™»é™†:\n```bash\nwandb login\n```\næ¨¡å‹è®­ç»ƒå¼€å§‹åå»[å®˜ç½‘](https://wandb.ai/)æŸ¥çœ‹è®­ç»ƒå®å†µ\n</details>\n\n\n\n\n## âš¡&nbsp;&nbsp; å¼€å§‹ä»»åŠ¡\n\n<details>\n<summary><b>æ–‡æœ¬åˆ†ç±»</b></summary>\n\n> æ•°æ®å¤„ç†\n```python\nfrom nlhappy.utils.make_doc import Doc, DocBin\nfrom nlhappy.utils.make_dataset import DatasetDict\n# æ„å»ºcorpus\n# å°†æ•°æ®å¤„ç†ä¸ºç»Ÿä¸€çš„Docå¯¹è±¡,å®ƒå­˜å‚¨ç€æ‰€æœ‰æ ‡ç­¾æ•°æ®\ndocs = []\n# dataä¸ºä½ è‡ªå·±çš„æ•°æ®\n# doc._.label ä¸ºæ–‡æœ¬çš„æ ‡ç­¾,ä¹‹æ‰€ä»¥åŠ \'_\'æ˜¯å› ä¸ºè¿™æ˜¯spacy Docä¿å­˜ç”¨æˆ·è‡ªå·±æ•°æ®çš„ç”¨æ³•\nfor d in data:\n    doc = nlp(d[\'text\'])\n    doc._.label = d[\'label\']\n    docs.append(doc)\n# ä¿å­˜corpus,æ–¹ä¾¿åè¾¹badcaseåˆ†æ\ndb = DocBin(docs=docs, store_user_data=True)\n# æ–°é—»æ–‡æœ¬-Tag3ä¸ºä¿å­˜æ ¼å¼ç›®å½•,éœ€è¦æ›´æ¢ä¸ºè‡ªå·±çš„å½¢å¼\ndb.to_disk(\'corpus/TNEWS-Tag15/train.spacy\')\n# æ„å»ºæ•°æ®é›†,ä¸ºäº†è®­ç»ƒæ¨¡å‹\nds = convert_docs_to_tc_dataset(docs=docs)\n# ä½ å¯ä»¥å°†æ•°æ®é›†è½¬æ¢ä¸ºdataframeè¿›è¡Œå„ç§åˆ†æ,æ¯”å¦‚è·å–æ–‡æœ¬æœ€å¤§é•¿åº¦\ndf = ds.to_pandas()\nmax_length = df[\'text\'].str.len().max()\n# æ•°æ®é›†åˆ‡åˆ†\ndsd = train_val_split(ds, val_frac=0.2)\n# ä¿å­˜æ•°æ®é›†,æ³¨æ„è¦ä¿å­˜åˆ°datasets/ç›®å½•ä¸‹\ndsd.save_to_disk(\'datasets/TNEWS\')\n```\n> è®­ç»ƒæ¨¡å‹\n\n- ç¼–å†™è®­ç»ƒè„šæœ¬,scripts/train.sh\n```\nnlhappy \\\ndatamodule=text_classification \\\ndatamodule.dataset=TNEWS \\\ndatamodule.plm=hfl/chinese-roberta-wwm-ext \\\ndatamodule.batch_size=32 \\\nmodel=bert_tc \\\nmodel.lr=3e-5 \\\nseed=1234\n# é»˜è®¤ä¸ºå•gpu 0å·æ˜¾å¡è®­ç»ƒ,å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¿®æ”¹æ˜¾å¡\n# trainer.devices=[1]\n# å•å¡åŠç²¾åº¦è®­ç»ƒ\n# trainer.precision=16\n# ä½¿ç”¨wandbè®°å½•æ—¥å¿—\n# logger=wandb\n# å¤šå¡è®­ç»ƒ\n# trainer=ddp trainer.devices=4\n```\n\n- åå°è®­ç»ƒ\n```\nnohup bash scripts/train.sh >/dev/null 2>&1 &\n```\n- å¦‚æœè®¾ç½®logger=wandbåˆ™ç°åœ¨å¯ä»¥å»[wandbå®˜ç½‘](https://wandb.ai/)æŸ¥çœ‹è®­ç»ƒè¯¦æƒ…äº†, å¹¶ä¸”ä¼šè‡ªåŠ¨äº§ç”Ÿlogsç›®å½•é‡Œé¢åŒ…å«äº†è®­ç»ƒçš„ckpt,æ—¥å¿—ç­‰ä¿¡æ¯.\n\n> æ„å»ºè‡ªç„¶è¯­è¨€å¤„ç†æµç¨‹,å¹¶æ·»åŠ ç»„ä»¶\n```python\nimport nlhappy\n\nnlp = nlhappy.nlp()\n# é»˜è®¤device cpu, é˜ˆå€¼0.8\nconfig = {\'device\':\'cuda:0\', \'threshold\':0.9}\ntc = nlp.add_pipe(\'text_classifier\', config=config)\n# logsæ–‡ä»¶å¤¹é‡Œé¢è®­ç»ƒçš„æ¨¡å‹è·¯å¾„\nckpt = \'logs/experiments/runs/TNEWS/date/checkpoints/epoch_score.ckpt/\'\ntc.init_model(ckpt)\ntext = \'æ–‡æœ¬\'\ndoc = nlp(text)\n# æŸ¥çœ‹ç»“æœ\nprint(doc.text, doc._.label, doc.cats)\n# ä¿å­˜æ•´ä¸ªæµç¨‹\nnlp.to_disk(\'path/nlp\')\n# åŠ è½½\nnlp = nlhappy.load(\'path/nlp\')\n```\n> badcaseåˆ†æ\n```python\nimport nlhappy\nfrom nlhappy.utils.make_doc import get_docs_form_docbin\nfrom nlhappy.utils.analysis_doc import analysis_text_badcase, Example\n\ntargs = get_docs_from_docbin(\'corpus/TNEWS-Tag15/train.spacy\')\nnlp = nlhappy.load(\'path/nlp\')\npreds = []\nfor d in targs:\n    doc = nlp(d[\'text\'])\n    preds.append(doc)\neg = [Example(x,y) for x,y in zip(preds, targs)]\nbadcases, score = analysis_text_badcase(eg, return_prf=True)\nprint(badcases[0].x, badcases[0].x._.label)\nprint(badcases[0].y, badcases[0].y._.label)\n```\n> éƒ¨ç½²\n- ç›´æ¥ç”¨nlpå¼€å‘æ¥å£éƒ¨ç½²\n- è½¬ä¸ºonnx\n```python\nfrom nlhappy.models import BertTextClassification\nckpt = \'logs/path/ckpt\'\nmodel = BertTextClassification.load_from_ckeckpoint(ckpt)\nmodel.to_onnx(\'path/tc.onnx\')\nmodel.tokenizer.save_pretrained(\'path/tokenizer\')\n```\n</details>\n\n<details>\n<summary><b>å®ä½“æŠ½å–</b></summary>\n\nnlhappyæ”¯æŒåµŒå¥—å’ŒéåµŒå¥—å®ä½“æŠ½å–ä»»åŠ¡\n> æ•°æ®å¤„ç†\n```python\nfrom nlhappy.utils.convert_doc import convert_spans_to_dataset\nfrom nlhappy.utils.make_doc import get_docs_from_docbin\nfrom nlhappy.utils.make_dataset import train_val_split\nimport nlhappy\n# åˆ¶ä½œdocs\nnlp = nlhappy.nlp()\ndocs = []\n# dataä¸ºä½ è‡ªå·±æ ¼å¼çš„åŸå§‹æ•°æ®,æŒ‰éœ€ä¿®æ”¹\n# åªéœ€è®¾ç½®doc.ents \n# åµŒå¥—å‹å®ä½“è®¾ç½®doc.spans[\'all\']\nfor d in data:\n    doc = nlp(d[\'text\'])\n    # éåµŒå¥—å®ä½“\n    ents = []\n    for ent in d[\'spans\']:\n        start = ent[\'start\']\n        end = ent[\'end\']\n        label = ent[\'label\']\n        span = doc.char_span(start, end, label)\n        ents.append(span)\n    doc.set_ents(ents)\n    docs.append(doc)\n    # åµŒå¥—å‹å®ä½“\n    for ent in d[\'spans\']:\n        start = ent[\'start\']\n        end = ent[\'end\']\n        label = ent[\'label\']\n        span = doc.char_span(start, end, label)\n        doc.spans[\'all\'].append(span)\n    docs.append(doc)\n# ä¿å­˜docs,æ–¹ä¾¿åè¾¹badcaseåˆ†æ\ndb = DocBin(docs=docs, store_user_data=True)\n# åˆ¶ä½œæ•°æ®é›†\n# å¦‚æœæ–‡æœ¬è¿‡é•¿å¯ä»¥è®¾ç½®å¥å­çº§åˆ«æ•°æ®é›†\nds = convert_spans_to_dataset(docs, sentence_level=False)\ndsd = train_val_split(ds, val_frac=0.2)\n# å¯ä»¥è½¬æ¢ä¸ºdataframeåˆ†ææ•°æ®\ndf = dsd.to_pandas()\nmax_length = df[\'text\'].str.len().max()\n# ä¿å­˜æ•°æ®é›†,æ³¨æ„è¦ä¿å­˜åˆ°datasets/ç›®å½•ä¸‹\ndsd.save_to_disk(\'datasets/your_dataset_name\')\n```\n> è®­ç»ƒæ¨¡å‹\nç¼–å†™è®­ç»ƒè„šæœ¬\n- å•å¡\n```bash\nnlhappy \\\ndatamodule=span_classification \\\ndatamodule.dataset=your_dataset_name \\\ndatamodule.max_length=2000 \\\ndatamodule.batch_size=2 \\\ndatamodule.plm=roberta-wwm-base \\\nmodel=global_pointer \\\nmodel.lr=3e-5 \\\nseed=22222\n```\n- å¤šå¡\n```\nnlhappy \\\ntrainer=ddp \\\ndatamodule=span_classification \\\ndatamodule.dataset=dataset_name \\\ndatamodule.max_length=350 \\\ndatamodule.batch_size=2 \\\ndatamodule.plm=roberta-wwm-base \\\nmodel=global_pointer \\\nmodel.lr=3e-5 \\\nseed=22222\n```\n- åå°è®­ç»ƒ\n```\nnohup bash scripts/train.sh >/dev/null 2>&1 &\n```\n- ç°åœ¨å¯ä»¥å»[wandbå®˜ç½‘](https://wandb.ai/)æŸ¥çœ‹è®­ç»ƒè¯¦æƒ…äº†, å¹¶ä¸”ä¼šè‡ªåŠ¨äº§ç”Ÿlogsç›®å½•é‡Œé¢åŒ…å«äº†è®­ç»ƒçš„ckpt,æ—¥å¿—ç­‰ä¿¡æ¯.\n> æ„å»ºè‡ªç„¶è¯­è¨€å¤„ç†æµç¨‹,å¹¶æ·»åŠ ç»„ä»¶\n```python\nimport nlhappy\n\nnlp = nlhappy.nlp()\n# é»˜è®¤device cpu, é˜ˆå€¼0.8\nconfig = {\'device\':\'cuda:0\', \'threshold\':0.9, \'set_ents\':True}\ntc = nlp.add_pipe(\'span_classifier\', config=config)\n# logsæ–‡ä»¶å¤¹é‡Œé¢è®­ç»ƒçš„æ¨¡å‹è·¯å¾„\nckpt = \'logs/experiments/runs/your_best_ckpt_path\'\ntc.init_model(ckpt)\ntext = \'æ–‡æœ¬\'\ndoc = nlp(text)\n# æŸ¥çœ‹ç»“æœ\n# doc.ents ä¸ºéåµŒå¥—å®ä½“,å¦‚æœæœ‰åµŒå¥—ä¼šé€‰æœ€å¤§è·¨åº¦å®ä½“\n# doc.spans[\'all\'] å¯ä»¥åŒ…å«åµŒå¥—å®ä½“\nprint(doc.text, doc.ents, doc.spans[\'all\'])\n# ä¿å­˜æ•´ä¸ªæµç¨‹\nnlp.to_disk(\'path/nlp\')\n# åŠ è½½\nnlp = nlhappy.load(\'path/nlp\')\n```\n> badcaseåˆ†æ\n```python\nimport nlhappy\nfrom nlhappy.utils.analysis_doc import analysis_ent_badcase, Example, analysis_span_badcase\nfrom nlhappy.utils.make_doc import get_docs_from_docbin\n\ntargs = get_docs_from_docbin(\'corpus/dataset_name/train.spacy\')\nnlp = nlhappy.load(\'path/nlp\')\npreds = []\nfor d in targs:\n    doc = nlp(d[\'text\'])\n    preds.append(doc)\neg = [Example(x,y) for x,y in zip(preds, targs)]\n# éåµŒå¥—å®ä½“\nbadcases, score = analysis_ent_badcase(eg, return_prf=True)\nprint(badcases[0].x, badcases[0].x.ents)\nprint(badcases[0].y, badcases[0].y.ents)\n# åµŒå¥—å®ä½“\nbadcases, score = analysis_span_badcase(eg, return_prf=True)\nprint(badcases[0].x, badcases[0].x.spans[\'all\'])\nprint(badcases[0].y, badcases[0].y.spans[\'all\'])\n```\n> éƒ¨ç½²\n- ç›´æ¥ç”¨nlpå¼€å‘æ¥å£éƒ¨ç½²\n- è½¬ä¸ºonnx\n```python\nfrom nlhappy.models import GlobalPointer\nckpt = \'logs/path/ckpt\'\nmodel = GlobalPointer.load_from_ckeckpoint(ckpt)\nmodel.to_onnx(\'path/tc.onnx\')\nmodel.tokenizer.save_pretrained(\'path/tokenizer\')\n```\n</details>\n\n<details>\n<summary><b>å®ä½“æ ‡å‡†åŒ–</b></summary>\nTODO\n</details>\n\n<details>\n<summary><b>å…³ç³»æŠ½å–</b></summary>\nTODO\n</details>\n\n<details>\n<summary><b>äº‹ä»¶æŠ½å–</b></summary>\nTODO\n</details>\n\n<details>\n<summary><b>é€šç”¨ä¿¡æ¯æŠ½å–</b></summary>\nTODO\n</details>\n\n<details>\n<summary><b>æ‘˜è¦</b></summary>\nTODO\n</details>\n\n<details>\n<summary><b>ç¿»è¯‘</b></summary>\nTODO\n</details>',
    'author': 'wangmengdi',
    'author_email': '790990241@qq.om',
    'maintainer': None,
    'maintainer_email': None,
    'url': None,
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'extras_require': extras_require,
    'entry_points': entry_points,
    'python_requires': '>=3.7',
}


setup(**setup_kwargs)
