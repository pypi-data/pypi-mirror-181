
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="english">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=cp1252" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Performance Curve</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="next" title="Predictions" href="predictions.html" />
    <link rel="prev" title="Confusion Matrix" href="confusionmatrix.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="performance-curve">
<h1>Performance Curve</h1>
<p>Construct and display a performance curve from the evaluation of classifiers.</p>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li>Evaluation Results: results of testing classification algorithms</li>
</ul>
<p><strong>Outputs</strong></p>
<ul class="simple">
<li>Calibrated Model: trained model using the calibrated learner</li>
</ul>
<p>The <strong>Performance Curve</strong> shows the curves for analysing the proportion of true positive data instances in relation to the classifier's threshold or the number of instances that we classify as positive. It offers three types of performance curves: <a class="reference external" href="https://en.wikipedia.org/wiki/Lift_(data_mining)" target="_blank">lift curve</a>, <a class="reference external" href="http://mlwiki.org/index.php/Cumulative_Gain_Chart" target="_blank">cumulative gains</a>, and <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html" target="_blank">precision-recall curve</a>.</p>
<p><strong>Lift curve</strong> shows the ratio between lift (the proportion of true positive instances to all positive instances in the prediction) and the proportion of positive instances. The higher the initial curve and the longer it is flat, the better the model. See <a class="reference external" href="https://medium.com/analytics-vidhya/understanding-lift-curve-b674d21e426" target="_blank">a tutorial for more details</a>.</p>
<p><strong>Cumulative gains</strong> chart shows the ratio of true positive instances (for example, people with heart disease) and support, which is the fraction of positively predicted instances (the ratio of patients with a heart disease in the prediction), assuming that the instances are ordered according to the model's probability of being positive (e.g. how likely the person has the disease). The greater the area between the curve and the baseline (dashed diagonal line), the better the model.</p>
<p><strong>Precision-recall curve</strong> shows the ratio between precision (ratio of true positives in positive predictions) and recall (ratio of true positives in positive class) at different thresholds. Ideally one aims at a high area under the curve.</p>
<p><img alt="../../_images/PerformanceCurve.png" src="../../_images/PerformanceCurve.png" /></p>
<ol class="simple">
<li>Choose the desired <em>Target class</em>. The default is chosen alphabetically. Choose whether to observe lift curve, cumulative gains or precision-recall.</li>
<li>If test results contain more than one classifier, the user can choose which curves she or he wants to see plotted. Click on a classifier to select or deselect the curve.</li>
<li><em>Show thresholds</em> plots a vertical dashed threshold line. The line represent at which probability threshold the prediction is considered positive. The line can be dragged left or right to change the threshold. <em>Show points</em> shows individual predictions as points on a plot. This option shows how many points were found at each value of x.</li>
<li>If <em>Apply Automatically</em> is ticked, changes are communicated automatically. Alternatively, click <em>Apply</em>.</li>
<li>A plot with the performance curve. The vertical dashed line represents the probability threshold and can be moved interactively. The diagonal dashed line in <strong>cumulative gains</strong> represents a baseline classifier.</li>
</ol>
<div class="section" id="examples">
<h2>Examples</h2>
<p>The widgets that provide the right type of the signal needed by the <strong>Performance Curve</strong> (evaluation data) are <a class="reference internal" href="testandscore.html"><span class="doc">Test and Score</span></a> and <a class="reference internal" href="predictions.html"><span class="doc">Predictions</span></a>.</p>
<p>In the first example, we observe the lift curve and cumulative gain for the <em>iris</em> data, where the classification goal is to predict the type of iris based on the measurements of the flower. We run <a class="reference internal" href="../model/logisticregression.html"><span class="doc">Logistic Regression</span></a> and <a class="reference internal" href="../model/randomforest.html"><span class="doc">Random Forest</span></a> in the <a class="reference internal" href="testandscore.html"><span class="doc">Test and Score</span></a> widget and send the results to <strong>Performance Curve</strong> to see their performance against a random model. Of the two algorithms tested, logistic regression outperforms the random forest. The curve tells us that by picking the first 34% of irises as ranked by the model and setting the probability threshold at 0.276, we are going to retain a perfect lift.</p>
<p><img alt="../../_images/PerformanceCurve-Example1.png" src="../../_images/PerformanceCurve-Example1.png" /></p>
<p>In the second example, we show how to calibrate a model in the <strong>Performance Curve</strong> widget. We are using the <em>heart-disease</em> data. First, the widget requires a single model on the input. This means cross-validation from Test and Score won't work, but there are as many models as there are folds. To pass a single model, use the <em>Test on test data</em> option.</p>
<p>In Performance Curve, we then observe the curve for the positive (1) class. The model has the optimal balance between precision and recall at the probability threshold of 0.475. We select this threshold and the model with the given threshold is sent to the output.</p>
<p>We can use this model in <a class="reference internal" href="predictions.html"><span class="doc">Predictions</span></a> to predict on new data with the calibrated model. See also <a class="reference internal" href="../model/calibratedlearner.html"><span class="doc">Calibrated Learner</span></a> for more calibration options.</p>
<p><img alt="../../_images/PerformanceCurve-Example2.png" src="../../_images/PerformanceCurve-Example2.png" /></p>
</div>
</div>


          </div>
          
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2015, Orange Data Mining.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/widgets/evaluate/performancecurve.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>