import numpy as np
from numpy.linalg import norm
from numpy import zeros, sum, abs, min



def PenCF( obj_fun,  manifold, Xinit = None, beta = None, penc_radius = 1.01, maxit= 100, gtol = 1e-5, post_process = True, verbosity = 2, **kwargs):
    """
    First-order method for optimization problem
    over the Stiefel manifoldm, derived from from the exact penalty model with compact and convex constraint 

    Arguments: 
    * obj_fun: callable
        The objective function, which should be called by ``fval, grad = obj_fun(X)`` 
    
    * manifold: STOP manifold class
        The manifold class provides essential functions for the solvers that could be defined by 
            from pystop.manifold import Stiefel 
            manifold = Stiefel(1000,10) 

    * Xinit: numpy ndarray in shape (n,p)
        optional, Starting point of the solver (better if it is on the manifold). 
        If none then a starting point will be generated by manifold.Init_point().


    Outputs:
        X: numpy ndarray in shape (n,p)
            The final results of the solver
        output_dict: dict 
            The dictionary that contains essential informations. 
    """
    kkts = []
    feas = []
    fvals = []

    if Xinit is None:
        Xinit = manifold.Init_point()



    X = Xinit

    X_p = Xinit

    fval, gradf = obj_fun(X)


    if beta is None:
        beta = 0.1*np.sqrt(np.sum(gradf **2))

    gradr = manifold.JA(X, gradf) + beta * manifold.JC(X, manifold.C(X))
    L = np.sqrt(np.sum(gradf **2)) + np.sqrt(np.sum(gradr **2))
    feasibility = manifold.Feas_eval(X)
   


    for jj in range(maxit):

        if jj < 3:
            stepsize = 0.01/L
        else:
            stepsize = abs( sum( S * Y ) / sum( Y* Y ) )
            stepsize = min( (stepsize, 1e10) )

        X_p = X

        X = X - stepsize * gradr

        X = manifold.Feas_correction(X)

        S = X - X_p

        fval, gradf = obj_fun(X)
        gradr_p = gradr
        gradr = manifold.JA(X, gradf) + beta * manifold.JC(X, manifold.C(X))
        Y = gradr - gradr_p

        substationarity = np.sqrt(np.sum(gradr **2))
        feasibility = manifold.Feas_eval(X)

        if verbosity == 2 and np.mod(jj,20) == 0:
            print("Iter:{}    fval:{:.3e}   kkts:{:.3e}    feas:{:3e}".format(jj,fval, substationarity, feasibility))

        kkts.append( substationarity )
        feas.append( feasibility )
        fvals.append( fval )



        if substationarity < gtol:
            if verbosity >= 1:
                print("Iter:{}    fval:{:.3e}   kkts:{:.3e}    feas:{:3e}".format(jj,fval, substationarity, feasibility))

            break

    if post_process:
        X = manifold.Post_process(X)
        fval, gradf = obj_fun(X)
        gradr = manifold.JA(X, gradf)
        substationarity = np.sqrt(np.sum(gradr **2))
        feasibility = manifold.Feas_eval(X)

        if verbosity >= 1:
            print("Post-processing")
            print("Iter:{}    fval:{:.3e}   kkts:{:.3e}    feas:{:3e}".format(jj,fval, substationarity, feasibility))


        kkts[-1] = substationarity
        feas[-1] = feasibility
        fvals[-1] = fval


    output_dict = { 'kkts': kkts, 'fvals': fvals, 'fea': feasibility, 'kkt': substationarity, 'fval': fval, 'feas': feas}

    return X, output_dict





def PCAL(obj_fun, manifold, Xinit = None, beta = None, maxit= 100, gtol = 1e-5, post_process = True, verbosity = 2, **kwargs):



    if Xinit is None:
        Xinit = manifold.Init_point()

    kkts = []
    feas = []
    fvals = []

    


    n = manifold._n
    p = manifold._p

    X = Xinit

    X_p = np.zeros( (n,p) )

    fval, gradf = obj_fun(X)

    
    if beta == None:
        beta = 0.1* np.linalg.norm(gradf,'fro')
    



    
    gradr = manifold.JA(X, gradf) + beta * manifold.JC(X, manifold.C(X))


    L = np.linalg.norm(gradf,'fro') + np.linalg.norm(gradr,'fro')

    for jj in range(maxit):

        

        

        if jj < 3:
            stepsize = 0.01/L
        else:
            stepsize = np.abs( np.sum( S * Y ) / np.sum( Y* Y ) )
            stepsize = np.min( (stepsize, 1e10) )

        X_p = X

        X = X - stepsize * gradr

        XX = X.T @ X
        feas_tmp = manifold.Feas_eval(X)
        if feas_tmp > 1e-1:
            if feas_tmp < 0.5:
                X = 1.5 * X - X @ (XX /2)
            else:
                X = np.linalg.solve( ( XX + np.eye(p) ) /2, X.T  ).T
        
        X = X / np.sqrt(np.sum(X **2, axis= 0, keepdims= True))

        S = X - X_p

        fval, gradf = obj_fun(X)
        gradr_p = gradr
        gradr = manifold.JA(X, gradf) + beta * manifold.JC(X, manifold.C(X))
        Y = gradr - gradr_p

        substationarity = np.linalg.norm(gradr, 'fro')
        feasibility = manifold.Feas_eval(X)

        if verbosity == 2 and np.mod(jj,20) == 0:
            print("Iter:{}    fval:{:.3e}   kkts:{:.3e}    feas:{:3e}".format(jj,fval, substationarity, feasibility))

        kkts.append( substationarity )
        feas.append( feasibility )
        fvals.append( fval )



        if substationarity < gtol:
            if verbosity >= 1:
                print("Iter:{}    fval:{:.3e}   kkts:{:.3e}    feas:{:3e}".format(jj,fval, substationarity, feasibility))

            break

    if post_process:
        X = manifold.Post_process(X)
        fval, gradf = obj_fun(X)
        gradr = manifold.JA(X, gradf)
        substationarity = np.linalg.norm(gradr, 'fro')
        feasibility = manifold.Feas_eval(X)

        if verbosity >= 1:
            print("Post-processing")
            print("Iter:{}    fval:{:.3e}   kkts:{:.3e}    feas:{:3e}".format(jj,fval, substationarity, feasibility))


        kkts[-1] = substationarity
        feas[-1] = feasibility
        fvals[-1] = fval


    output_dict = { 'kkts': kkts, 'fvals': fvals, 'fea': feasibility, 'kkt': substationarity, 'fval': fval, 'feas': feas}

    return X, output_dict


# def penc_safeguard(X, radius, feas):
#     n,p = np.shape(X)

#     # r_X = np.sum( X**2 )
#     # if  r_X> radius:
#     #     X = X / np.sqrt(r_X/ radius)

#     if feas > 5e-1 :
#         X = np.linalg.solve( X.T @ X + np.eye(p), 2*X.T ).T
#     elif feas > 1e-2:
#         X = 1.5 * X - 0.5 * X@(X.T @ X) 

#     return X 
    