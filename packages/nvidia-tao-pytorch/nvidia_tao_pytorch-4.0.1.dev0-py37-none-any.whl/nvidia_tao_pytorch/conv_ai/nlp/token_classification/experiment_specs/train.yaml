# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.
# TLT Spec file for training of the BERT model on a Token Classification task:
# Named Entity Recognition on GMB dataset

trainer:
  max_epochs: 5

tlt_checkpoint_interval: 1

model:
  tokenizer:
    tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
    vocab_file: null # path to vocab file
    tokenizer_model: null # only used if tokenizer is sentencepiece
    special_tokens: null

  language_model:
    pretrained_model_name: bert-base-uncased
    lm_checkpoint: null
    config_file: null # json file, precedence over config
    config: null
    nemo_file: null

  head:
    num_fc_layers: 2
    fc_dropout: 0.5
    activation: 'relu'
    use_transformer_init: True

  # Path to file with label_ids, generated with dataset_convert.py.
  # Those labels are used by the model as labels (names of target classes, their number).
  label_ids: ???

# Path to directory containing both finetuning and validation data.
data_dir: ???

training_ds:
  text_file: text_train.txt
  labels_file: labels_train.txt
  batch_size: 64

validation_ds:
  text_file: text_dev.txt
  labels_file: labels_dev.txt
  batch_size: 64

optim:
  name: adam
  lr: 5e-5
  weight_decay: 0.00

  # scheduler setup
  sched:
    name: WarmupAnnealing
    # Scheduler params
    warmup_steps: null
    warmup_ratio: 0.1
    last_epoch: -1
    # pytorch lightning args
    monitor: val_loss
    reduce_on_plateau: false

