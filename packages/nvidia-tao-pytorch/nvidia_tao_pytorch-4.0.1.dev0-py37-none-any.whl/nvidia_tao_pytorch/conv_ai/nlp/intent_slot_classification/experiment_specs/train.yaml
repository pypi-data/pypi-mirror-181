# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
# TLT Spec file for training of the BERT model on text classification task using the SST-2 dataset.
  
# Name of the file where trained model will be saved.
save_to: trained-model.tlt

tlt_checkpoint_interval: 1

optim:
  name: adam
  lr: 2e-5
  # optimizer arguments
  betas: [0.9, 0.999]
  weight_decay: 0.01

  # scheduler setup
  sched:
    name: WarmupAnnealing
    # Scheduler params
    warmup_steps: null
    warmup_ratio: 0.1
    last_epoch: -1
    # pytorch lightning args
    monitor: val_loss
    reduce_on_plateau: false

model:
  class_balancing: null # choose from [null, weighted_loss]. weighted_loss enables the weighted class balancing of the loss, may be used for handling unbalanced classes
  intent_loss_weight: 0.6 # relation of intent to slot loss in total loss (between 0 to 1)
  pad_label: -1 # if -1 not slot token will be used
  ignore_extra_tokens: false
  ignore_start_end: true # do not use first and last token for slot training

  tokenizer:
      tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
      vocab_file: null # path to vocab file 
      tokenizer_model: null # only used if tokenizer is sentencepiece
      special_tokens: null

  language_model:
    max_seq_length: 50
    pretrained_model_name: bert-base-uncased
    lm_checkpoint: null
    config_file: null # json file, precedence over config
    config: null

  head:
    num_output_layers: 2
    fc_dropout: 0.1

data_dir: ???

training_ds:
  prefix: train
  batch_size: 32
  num_workers: 2

validation_ds:
  prefix: test
  batch_size: 32
  num_workers: 2

