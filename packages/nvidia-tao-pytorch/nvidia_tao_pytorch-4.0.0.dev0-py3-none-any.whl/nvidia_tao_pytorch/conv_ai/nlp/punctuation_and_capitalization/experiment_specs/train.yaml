# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.
# TLT Spec file for training of the BERT model on text classification task using the SST-2 dataset.

trainer:
  max_epochs: 5

tlt_checkpoint_interval: 1

model:
  class_labels:
    punct_labels_file: punct_label_ids.csv
    capit_labels_file: capit_label_ids.csv

  punct_label_ids:
    O: 0
    ',': 1
    '.': 2
    '?': 3

  capit_label_ids:
    O: 0
    U: 1

  tokenizer:
    tokenizer_name: ${language_model.pretrained_model_name} # or sentencepiece
    vocab_file: null # path to vocab file
    tokenizer_model: null # only used if tokenizer is sentencepiece
    special_tokens: null

  language_model:
    pretrained_model_name: bert-base-uncased
    lm_checkpoint: null
    config_file: null # json file, precedence over config
    config: null

  punct_head:
    punct_num_fc_layers: 1
    fc_dropout: 0.1
    activation: 'relu'
    use_transformer_init: true

  capit_head:
    capit_num_fc_layers: 1
    fc_dropout: 0.1
    activation: 'relu'
    use_transformer_init: true

  optim:
    name: adam
    lr: 1e-5
    weight_decay: 0.00

    sched:
      name: WarmupAnnealing
      # Scheduler params
      warmup_steps: null
      warmup_ratio: 0.1
      last_epoch: -1

      # pytorch lightning args
      monitor: val_loss
      reduce_on_plateau: false

data_dir: ???

training_ds:
  text_file: text_train.txt
  labels_file: labels_train.txt
  shuffle: true
  num_samples: -1 # number of samples to be considered, -1 means the whole the dataset
  batch_size: 64
  ds_item: ???
  tokens_in_batch: 15000

validation_ds:
  text_file: text_dev.txt
  labels_file: labels_dev.txt
  shuffle: false
  num_samples: -1 # number of samples to be considered, -1 means the whole the dataset
  batch_size: 64
  ds_item: ???
  tokens_in_batch: 15000
