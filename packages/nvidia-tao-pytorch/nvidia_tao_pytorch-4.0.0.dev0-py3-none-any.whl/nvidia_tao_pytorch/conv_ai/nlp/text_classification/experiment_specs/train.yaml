# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.
# TLT Spec file for training of a BERT model on text classification tasks.

trainer:
  max_epochs: 100

tlt_checkpoint_interval: 1

model:
  # Labels that will be used to "decode" predictions.
  class_labels:
    class_labels_file : null # optional to specify a file containing the list of the labels

  tokenizer:
    tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
    vocab_file: null # path to vocab file
    tokenizer_model: null # only used if tokenizer is sentencepiece
    special_tokens: null

  language_model:
    pretrained_model_name: bert-base-uncased
    lm_checkpoint: null
    config_file: null # json file, precedence over config
    config: null 
    nemo_file: null

  classifier_head:
    # This comes directly from number of labels/target classes.
    num_output_layers: 2
    fc_dropout: 0.1


training_ds:
  file_path: ???
  batch_size: 64
  shuffle: true
  num_samples: -1 # number of samples to be considered, -1 means all the dataset
  num_workers: 3
  drop_last: false
  pin_memory: false

validation_ds:
  file_path: ???
  batch_size: 64
  shuffle: false
  num_samples: -1 # number of samples to be considered, -1 means all the dataset
  num_workers: 3
  drop_last: false
  pin_memory: false

optim:
  name: adam
  lr: 2e-5
  # optimizer arguments
  betas: [0.9, 0.999]
  weight_decay: 0.01

  # scheduler setup
  sched:
    name: WarmupAnnealing
    # Scheduler params
    warmup_steps: null
    warmup_ratio: 0.1
    last_epoch: -1
    # pytorch lightning args
    monitor: val_loss
    reduce_on_plateau: false
